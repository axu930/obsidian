# Prompt

Hi, I would like to know more about methods used by researchers to quantify nonlinearity in the context of deep neural networks. Are there any theoretical guarantees known in machine learning literature on how we can quantify how flexible a model is to varied data based off the model design? Please include original sources and quotes if possible in the final report.

I am mostly interested in nonlinearity in the context of generalization capacity, and model robustness to noisy data. Please cover all types of architectures such as transformers and feedforward nets, as well as more niche ones like polynomial nets. The motivation behind this question is that I would like a way to quantify a priori lower and upper bounds for the performance of a given machine learning framework to determine whether the empirical performance is potentially better than what we might expect.

# Sources 

- **Philipp, G. & Carbonell, J. (2019)**  
    _The Nonlinearity Coefficient – A Practical Guide to Neural Network Nonlinearity Measurement._  
    [arXiv:1812.10032](https://arxiv.org/abs/1812.10032)
    
- **Bouniot, Q., Glaude, T., Le Roux, N., & Scieur, D. (2023)**  
    _Nonlinearity in Deep Learning: A Geometric Perspective._  
    [arXiv:2306.09365](https://arxiv.org/abs/2306.09365)
    
- **Montúfar, G. F., Pascanu, R., Cho, K., & Bengio, Y. (2014)**  
    _On the Number of Linear Regions of Deep Neural Networks._  
    [arXiv:1402.1869](https://arxiv.org/abs/1402.1869)
    
- **Novak, R., Bahri, Y., Abolafia, D. A., Pennington, J., & Sohl-Dickstein, J. (2018)**  
    _Sensitivity and Generalization in Neural Networks: An Empirical Study._  
    [arXiv:1802.08760](https://arxiv.org/abs/1802.08760)
    
- **Kileel, J., Trager, M., & Shmakov, A. (2019)**  
    _Expressive Power of Deep Polynomial Neural Networks._  
    [arXiv:1905.12217](https://arxiv.org/abs/1905.12217)
    
- **Sarkar, S., Zhang, X., Arpit, D., & Bengio, Y. (2019)**  
    _Enforcing Linearity in Deep Networks Improves Robustness._  
    [arXiv:1906.06762](https://arxiv.org/abs/1906.06762)
    
- **Bartlett, P. L., Foster, D. J., & Telgarsky, M. J. (2017)**  
    _Spectrally-Normalized Margin Bounds for Neural Networks._  
    [arXiv:1706.08498](https://arxiv.org/abs/1706.08498)

# Output

## Quantifying Nonlinearity in Deep Neural Networks

Deep neural networks (DNNs) derive their power from built-in nonlinearity, but excess nonlinearity can lead to overfitting and sensitivity to noise. Researchers have proposed several ways to **measure a network’s nonlinearity** and relate it to its capacity and robustness. Key approaches include gradient-based metrics, optimal-transport measures, combinatorial counts of linear regions, and algebraic-geometry expressivity. Many of these metrics come with theoretical underpinnings or empirical validation. Below we review the main methods and their implications for generalization and noise robustness.

## Nonlinearity Metrics and Measures

- **Nonlinearity Coefficient (NLC)**. Philipp & Carbonell (2019) define the _nonlinearity coefficient_ as a gradient-based scalar metric of network nonlinearity[ar5iv.org](https://ar5iv.org/pdf/1806.00179#:~:text=We%20introduce%20the%20nonlinearity%20coefficient,section%203). Intuitively, the NLC compares the average sensitivity of the network’s output (via the input-output Jacobian) to the variability of its output values. Formally it combines input/output covariance matrices with the Jacobian (see their Definition 2), yielding a simple quantity one can compute at initialization. Empirically, they find that the NLC in a randomly initialized _fully-connected_ network strongly predicts test error after training: “the NLC… is a powerful predictor of test error and that attaining a right-sized NLC is essential for achieving an optimal test error”[ar5iv.org](https://ar5iv.org/pdf/1806.00179#:~:text=We%20show%20that%20the%20NLC%2C,connected%20feedforward%20networks%20%28section%204). In other words, networks that are _too_ nonlinear (high NLC) or _too_ linear (low NLC) tend to generalize poorly. Crucially, the NLC is designed to be invariant to trivial rescalings of inputs or weights, making it a robust indicator. Philipp & Carbonell show theoretically that NLC is tied to the size of input regions that can be well-approximated linearly, and that it correlates with the information gained by a single gradient step[ar5iv.org](https://ar5iv.org/pdf/1806.00179#:~:text=network%20input%20and%20output%20with,to%20small%20random%20input%20perturbations).
    
- **Optimal-Transport Affinity Score and Nonlinearity Signature**. Bouniot et al. (2023) introduce an _affinity score_ ρ that quantifies how non-linear an activation is via optimal transport. Given a layer input distribution $X$ and post-activation output $σ(X)$, they compute the 2-Wasserstein distance between the true output and the best _affine_ map of the input. Normalizing this distance yields
    
    ρaff(X,σ(X))=1−W22(TaffX,σ(X))2 Tr[Σ(σ(X))].ρ_{\rm aff}(X,σ(X)) = 1 - \frac{W_2^2(T_{\rm aff}X,σ(X))}{2\,\mathrm{Tr}[\Sigma(σ(X))]}.ρaff​(X,σ(X))=1−2Tr[Σ(σ(X))]W22​(Taff​X,σ(X))​.
    
    By construction $ρ_{\rm aff}=1$ if $σ$ is linear (no cost) and $ρ_{\rm aff}=0$ if the output is maximally nonlinear (the affine fit is uninformative)[openreview.net](https://openreview.net/pdf/1b2680339856561b8343a8b01a8b8af4303014c0.pdf#:~:text=%CF%81aff,0%20when%20it%20is%20maximally). This _affinity score_ can be computed per layer/activation; the **nonlinearity signature** of a network is the vector of affinity scores for all activation layers[openreview.net](https://openreview.net/pdf/1b2680339856561b8343a8b01a8b8af4303014c0.pdf#:~:text=151%20Non,functions%20encountered%20across%20its%20layers). This signature reveals how nonlinearity propagates through the architecture. For example, applying this to modern vision models, the authors find that original Vision Transformers (ViTs) exhibit “highly non-linear” early layers (especially with small patches) that then become more linear later[ar5iv.org](https://ar5iv.org/pdf/2310.11439#:~:text=We%20now%20come%20to%20the,results%20remind%20us%20of%20Figure). They prove properties of ρ based on the Gelbrich bound: it is bounded in [0,1], interpretable, and directly measures deviation from the best affine fit[openreview.net](https://openreview.net/pdf/1b2680339856561b8343a8b01a8b8af4303014c0.pdf#:~:text=%CF%81aff,0%20when%20it%20is%20maximally). Importantly, this measure is _theoretically grounded_ (rooted in optimal transport) and is shown to capture nonlinearity in a way that correlates with architectural differences better than simpler proxies like linear CKA or weight norms[ar5iv.org](https://ar5iv.org/pdf/2310.11439#:~:text=We%20now%20aim%20to%20understand,score%20between%20the%20linear%20model).
    
- **Piecewise-Linear Expressivity (Linear Regions)**. For networks with piecewise-linear activations (ReLU, hardtanh, etc.), expressivity can be assessed by counting linear regions in input space. Pioneering work (e.g. Montúfar _et al._, 2014) shows that a $d$-layer ReLU net can partition $\mathbb{R}^n$ into exponentially many affine regions. Concretely, the _number of linear regions_ or the _number of distinct activation patterns_ has been used as a capacity measure. As one review notes, “for piecewise linear activation functions… the expressiveness measure is equal to either the number of the linear regions or the number of activation patterns”[ar5iv.org](https://ar5iv.org/pdf/2310.11439#:~:text=First%20such%20contributions%20considered%20specifically,2020). Although exact counting is hard in general, upper and lower bounds (and randomized estimates) on region counts have been derived for various architectures. A network capable of more linear pieces is effectively more nonlinear in behavior. While these counts quantify raw expressive power, relating them to generalization requires care: more regions imply higher complexity but can also fit complex data.
    
- **Jacobian Sensitivity and Lipschitz Bounds**. Another empirical approach is to measure _local sensitivity_ of the network output to input perturbations. A common proxy is the Frobenius or spectral norm of the input-output Jacobian $J(x)$ around data points. Novak _et al._ (2018) conducted large-scale experiments showing that networks with **smaller average Jacobian norm** (i.e. less sensitive to small input changes) tend to generalize better. They report that “trained neural networks are more robust to input perturbations… as measured by the input-output Jacobian… and that this correlates well with generalization”[openreview.net](https://openreview.net/forum?id=HJC2SzZCW#:~:text=image%20classification%20datasets,level%20of%20individual%20test%20points). Factors known to hurt generalization (like training on random labels or using full-batch SGD) led to larger Jacobian norms; by contrast, techniques that improve generalization (data augmentation, ReLU nonlinearities) yielded lower sensitivity. Thus the Jacobian norm acts as an _empirical nonlinearity measure_: a high norm means the network reacts strongly (nonlinearly) to noise. This insight also underlies theoretical robustness: controlling Lipschitz constants (which bound the Jacobian norm) provides guarantees on perturbation robustness.
    
- **Algebraic Expressivity (Polynomial Networks)**. For networks with polynomial activations (a less common but theoretically interesting class), algebraic-geometry offers metrics. Kileel _et al._ (2019) show that for a fixed architecture and activation degree, the set of functions representable by a deep polynomial net forms an algebraic variety in function space. They propose the **dimension** of this variety as a precise expressivity measure: “the dimension of this variety as a precise measure of the expressive power of polynomial neural networks”[openreview.net](https://openreview.net/pdf?id=Bye6JSHxLr#:~:text=variety%20as%20a%20precise%20measure,activation%20degrees%2C%20as%20well%20as). Practically, they derive exact formulas and bounds for this dimension in terms of layer widths and polynomial degree, establishing how architecture design constrains capacity. For instance, they prove explicit conditions under which the network’s function space “fills” the ambient polynomial space (so the network can realize any polynomial of that degree) or falls short. These geometric results give upper and lower bounds on what functions a polynomial network can learn, linking its **intrinsic nonlinearity (degree)** to capacity and hence to potential generalization.
    

## Architectures and Nonlinearity

- **Feedforward (Dense) Networks**: Measures like the NLC and linear-region counts have been primarily studied in fully-connected nets. Philipp & Carbonell’s NLC was validated on random fully-connected architectures (and shown to be independent of simple scaling choices)[ar5iv.org](https://ar5iv.org/pdf/1806.00179#:~:text=We%20introduce%20the%20nonlinearity%20coefficient,section%203). The Jacobian-based findings[openreview.net](https://openreview.net/forum?id=HJC2SzZCW#:~:text=image%20classification%20datasets,level%20of%20individual%20test%20points) apply to any differentiable net, but they emphasize that traditional ReLU-Multi-Layer-Perceptrons achieve better generalization via robustness to input noise. In practice, one can compute these metrics before training to anticipate performance: e.g., the NLC (at init) “can robustly predict poor training outcomes before training even begins”[ar5iv.org](https://ar5iv.org/pdf/1806.00179#:~:text=We%20introduce%20what%20is%2C%20to,outcomes%20before%20training%20even%20begins).
    
- **Convolutional and Vision Models**: Although many measures were first developed for MLPs, they extend to CNNs and vision models. The affinity/nonlinearity signature has been applied to dozens of convolutional nets (ResNets, DenseNets, etc.) and Vision Transformers. Bouniot _et al._ report that recent convnets tend to _reduce_ nonlinearity in later layers (often via architectural tricks like bottlenecks or gating), whereas the original vision transformers (ViT-Large/ViT-Huge) remain _highly nonlinear_ in early layers[ar5iv.org](https://ar5iv.org/pdf/2310.11439#:~:text=We%20now%20come%20to%20the,results%20remind%20us%20of%20Figure). In Figure 7 of their paper, ViTs with 16×16 patches show a steep drop in nonlinearity only in final layers, implying they preserve strong nonlinearity throughout. This suggests attention mechanisms can funnel activations into regimes where even simple (linear) activations behave nonlinearly. No specialized metric unique to Transformers is needed – the same affinity score or Jacobian norm can profile them. (Notably, Bouniot _et al._ cluster various models by their nonlinearity signatures and find meaningful groupings across CNNs and ViTs.)
    
- **Polynomial Neural Networks**: Polynomial nets (using e.g. $σ(x)=x^r$ or ratios of polynomials) have explicit degree-based nonlinearity. Their expressivity grows with activation degree $r$, but also risks overfitting. Kileel _et al._’s dimensionality results[openreview.net](https://openreview.net/pdf?id=Bye6JSHxLr#:~:text=variety%20as%20a%20precise%20measure,activation%20degrees%2C%20as%20well%20as) quantify this. For example, they show that increasing the polynomial degree dramatically raises the dimension of the representable function space (up to combinatorial limits). From a practical standpoint, such networks require careful design: higher-degree layers increase expressivity but demand more data to avoid overfitting. While explicit error bounds for polynomial nets are rare, their study of _filling_ vs _bottleneck_ conditions effectively gives architecture-dependent upper/lower bounds on function complexity.
    

## Theoretical Bounds and Capacity Measures

Several nonlinearity measures tie into classical generalization bounds:

- **Norm and Lipschitz-Based Bounds**: Spectral norm and product-of-norms bounds (Bartlett _et al._, 2017) control worst-case Lipschitz constants, indirectly limiting nonlinearity. These give margin-based guarantees: roughly, $\text{GenError}\le O\Big(\frac{\prod_i |W_i|}{\sqrt{n}}\Big)$ times a complexity factor. While not explicitly a “nonlinearity metric,” a small Lipschitz constant means the network behaves more linearly. Indeed, Sarkar _et al._ enforce near-linearity via a “Local Lipschitz regularizer” and find it further boosts robustness[ar5iv.org](https://ar5iv.org/pdf/1910.08108#:~:text=we%20show%20that%20by%20enforcing,Our%20method%20outperforms%20most). Theoretical capacity can also be studied via VC dimension or Rademacher complexity, but these depend on weights and network size rather than functional nonlinearity per se.
    
- **Algebraic and Geometric Bounds**: As noted, polynomial networks allow exact algebraic analysis. More generally, results on linear-region counts (e.g. using triangulations or zonotopes) yield upper bounds on how many regions a network with given width/depth can create. These bounds serve as proxies for expressivity. There are also recent “neurogeometry” approaches using coverings or manifold dimensionality to bound generalization. In all cases, the idea is that more nonlinearity (more regions or higher-dimensional function families) tends to increase capacity, which can be limited by stronger regularization or more data.
    
- **NTK and Linear Regime**: In the infinite-width limit, networks become essentially linear models (the Neural Tangent Kernel regime) with well-understood generalization through kernel theory. This regime corresponds to very _low_ effective nonlinearity (networks change little from init). Generalization bounds in this limit exist (e.g. via kernel complexity), but they do not readily extend to finite-width nonlinear networks. However, they underscore the tradeoff: as networks deviate from the linearized NTK regime (become more nonlinear during training), classical bounds become weaker, motivating new nonlinearity-specific metrics like NLC or signature.
    

## Nonlinearity, Generalization, and Robustness

Empirical and theoretical studies consistently find that **moderate nonlinearity** tends to maximize generalization, whereas extreme nonlinearity can harm it:

- _Generalization Correlations_: Both the NLC and Jacobian studies demonstrate a link to generalization. Philipp & Carbonell report that networks with optimal (neither too high nor too low) NLC show best test accuracy[ar5iv.org](https://ar5iv.org/pdf/1806.00179#:~:text=We%20show%20that%20the%20NLC%2C,connected%20feedforward%20networks%20%28section%204). Novak _et al._ observe that models with smaller Jacobian norms (hence locally more linear behavior) tend to generalize better[openreview.net](https://openreview.net/forum?id=HJC2SzZCW#:~:text=image%20classification%20datasets,level%20of%20individual%20test%20points). In practice, one might aim to control these metrics (via architecture or initialization) to improve performance. Notably, factors like batch normalization and skip connections often reduce effective nonlinearity, which may partly explain their success in aiding generalization.
    
- _Robustness to Noise_: Less nonlinear (more linear) networks are inherently more robust to input noise and adversarial perturbations. Sarkar _et al._ explicitly demonstrate this by adding a “linearity constraint” in training: networks trained to stay closer to linear functions achieve state-of-the-art adversarial accuracy on MNIST/CIFAR[ar5iv.org](https://ar5iv.org/pdf/1910.08108#:~:text=we%20show%20that%20by%20enforcing,Our%20method%20outperforms%20most). In their words, “enforcing a DNN to be linear in transformed input and feature space improves robustness significantly”[ar5iv.org](https://ar5iv.org/pdf/1910.08108#:~:text=we%20show%20that%20by%20enforcing,Our%20method%20outperforms%20most). Intuitively, if output changes smoothly with inputs (low nonlinearity), small input noise cannot easily flip predictions. Conversely, highly nonlinear models can be brittle: small input shifts may hit different linear regions or saturated activations, yielding large output changes.
    
- _Bounds on Performance_: Some nonlinearity measures come with provable performance bounds. For instance, if one bounds the Lipschitz constant or Jacobian norm, one can derive worst-case guarantees on error increase under noise. Algebraic results (e.g. polynomial net dimensions) translate to worst-case expressivity and thus to expected worst-case training error given limited data. However, tight lower bounds (i.e. proving a network _cannot_ generalize below a certain error given its nonlinearity) remain rare in the deep learning literature. Most current work provides empirical evidence or upper bounds (i.e. says how complex the network can be, not the actual error) rather than precise bounds on test error.
    

## Summary

In summary, researchers quantify DNN nonlinearity using a variety of tools. Gradient-based metrics like the **Nonlinearity Coefficient (NLC)** measure how much the network’s behavior deviates from linearity[ar5iv.org](https://ar5iv.org/pdf/1806.00179#:~:text=We%20introduce%20the%20nonlinearity%20coefficient,section%203). Optimal-transport measures like the **affinity score** yield a _nonlinearity signature_ describing each layer’s nonlinearity[openreview.net](https://openreview.net/pdf/1b2680339856561b8343a8b01a8b8af4303014c0.pdf#:~:text=%CF%81aff,0%20when%20it%20is%20maximally)[openreview.net](https://openreview.net/pdf/1b2680339856561b8343a8b01a8b8af4303014c0.pdf#:~:text=151%20Non,functions%20encountered%20across%20its%20layers). Classical measures count linear regions or activation patterns[ar5iv.org](https://ar5iv.org/pdf/2310.11439#:~:text=First%20such%20contributions%20considered%20specifically,2020). Sensitivity metrics (Jacobian norm) indirectly assess local nonlinearity[openreview.net](https://openreview.net/forum?id=HJC2SzZCW#:~:text=image%20classification%20datasets,level%20of%20individual%20test%20points). For polynomial nets, algebraic dimension captures expressivity[openreview.net](https://openreview.net/pdf?id=Bye6JSHxLr#:~:text=variety%20as%20a%20precise%20measure,activation%20degrees%2C%20as%20well%20as). Empirically, these metrics correlate with generalization – e.g. “networks with larger sensitivity [Jacobian norm] generalize worse”[openreview.net](https://openreview.net/forum?id=HJC2SzZCW#:~:text=image%20classification%20datasets,level%20of%20individual%20test%20points) – and formalize known guidelines: networks that are “just nonlinear enough” tend to work best. On robustness, the trend is clear: more linear networks are more robust to noise[ar5iv.org](https://ar5iv.org/pdf/1910.08108#:~:text=we%20show%20that%20by%20enforcing,Our%20method%20outperforms%20most)[openreview.net](https://openreview.net/forum?id=HJC2SzZCW#:~:text=image%20classification%20datasets,level%20of%20individual%20test%20points). Many of the above methods come with theoretical backing (e.g. bounds on region counts or leverage convexity arguments for linear parts) or at least well-founded definitions, allowing practitioners to predict model behavior _a priori_ based on design choices.

**Sources:** Key references include Philipp & Carbonell on the Nonlinearity Coefficient[ar5iv.org](https://ar5iv.org/pdf/1806.00179#:~:text=We%20introduce%20the%20nonlinearity%20coefficient,section%203), Bouniot _et al._ on the affine optimal-transport affinity score[openreview.net](https://openreview.net/pdf/1b2680339856561b8343a8b01a8b8af4303014c0.pdf#:~:text=%CF%81aff,0%20when%20it%20is%20maximally)[openreview.net](https://openreview.net/pdf/1b2680339856561b8343a8b01a8b8af4303014c0.pdf#:~:text=151%20Non,functions%20encountered%20across%20its%20layers)[ar5iv.org](https://ar5iv.org/pdf/2310.11439#:~:text=We%20now%20come%20to%20the,results%20remind%20us%20of%20Figure), Novak _et al._ on Jacobian sensitivity[openreview.net](https://openreview.net/forum?id=HJC2SzZCW#:~:text=image%20classification%20datasets,level%20of%20individual%20test%20points), Sarkar _et al._ on linearity vs robustness[ar5iv.org](https://ar5iv.org/pdf/1910.08108#:~:text=we%20show%20that%20by%20enforcing,Our%20method%20outperforms%20most), and Kileel _et al._ on polynomial-network expressivity[openreview.net](https://openreview.net/pdf?id=Bye6JSHxLr#:~:text=variety%20as%20a%20precise%20measure,activation%20degrees%2C%20as%20well%20as), among others.