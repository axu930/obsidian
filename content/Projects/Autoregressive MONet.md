Resources/references
- [Monet paper](https://arxiv.org/pdf/2401.17992)

There's a few ways we can approach this: 
- Standard Attention + Mu-layer (drop-in replacement for MLP)
- Attention with tanh instead of softmax 
- Can also try some other types of attention. 